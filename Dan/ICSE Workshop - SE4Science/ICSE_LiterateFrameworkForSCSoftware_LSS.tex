% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}

\usepackage{qtree}
\usepackage{listings}
\usepackage[final]{pdfpages}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{graphicx}

\newcommand{\lss}{SCS}
\newcommand{\fullname}{Sous-Chef de Software} 
%D because recipes and it's pretty much in charge of the kitchen

%D go full french? 
%D Sous-Chef de Logiciel, Sous-Chef de l'Informatique?

%D Other names (Literate can be precede pretty much any):
%D Literate Architect (ArchiTeXt lol), Archaeologist, Craftsman
%D Jack-of-All-Trades (JAT -- Opens us up to ``master of none'' critiques),
%D (Literate) Sync-Dev (SD/LSD) <-- I like the second acronym (Dan).
%D Encyclopedia Dev (don't like the acronym, but name plays on knowledge cap)
%D Quality Management Framework (QMF), get to use `Q'!
%D Tracer
%D ???

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


% DOI
%\doi{10.475/123_4} %D What goes here?

% ISBN
%\isbn{123-4567-24-567/08/06}  %D What goes here?

%Conference
\conferenceinfo{SE4SC}{?, 2016, Seattle, WA, USA}  %D What goes here?

\acmPrice{\$15.00}  %D What goes here?

%
% --- Author Metadata here ---
\conferenceinfo{SE4SC}{2016 ?, ? USA}  %D What goes here?
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.
% --- End of Author Metadata ---

\title{Position Paper: A Literate Framework %D or Tool?
for Scientific Software Development
%\titlenote{(Produces the permission block, and
%copyright information). For use with
%SIG-ALTERNATE.CLS. Supported by ACM.}}
%\subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}
}

\numberofauthors{3} 
\author{
%D No idea what the ordering of this section should be
\alignauthor
Dan Szymczak\\
       \affaddr{McMaster University}\\
       \affaddr{1280 Main Street W}\\
       \affaddr{Hamilton, Ontario}\\
       \email{szymczdm@mcmaster.ca}
\alignauthor
Spencer Smith\\
       \affaddr{McMaster University}\\
       \affaddr{1280 Main Street W}\\
       \affaddr{Hamilton, Ontario}\\
       \email{smiths at mcmaster.ca} 
%D Not sure if you want your email machine-readable
\alignauthor
Jacques Carette\\
       \affaddr{McMaster University}\\
       \affaddr{1280 Main Street W}\\
       \affaddr{Hamilton, Ontario}\\
       \email{carette at mcmaster.ca}
%D Not sure if you want your email machine-readable
}
\date{24 January 2016}

\maketitle
\begin{abstract}
Qualities of scientific computing software can be improved by following a
rational design process. The qualities of traceability, verifiability, and
reproducibility are of exceptionally high concern when attempting to certify
software. We have begun development of a framework, \fullname{} (\lss{}),
to help ensure complete traceability, adapt to the ever changing nature of
scientific computing projects, and ensure software artifacts can be easily and
quickly (re-)~generated as necessary.

Using an example-based approach to implement \lss{}, we have already seen many
benefits that it provides to the development process. \lss{} keeps all software
artifacts in sync with each other and the source, allows for reuse of common
concepts across projects, and aids in the verification of software. It is our
hope that \lss{} will lead to the development of higher quality software at low
cost.
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%

%TODO: THIS
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>  
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

\keywords{ACM proceedings; \LaTeX; text tagging}

\section{Introduction}

A rational document driven design process for Scientific Computing (SC) software
potentially improves the software qualities of verifiability, reliability,
usability, maintainability, reusability and
reproducibility~\cite{SmithAndKoothoor2016}.  However, many researchers have
reported that a document driven process is not used by, nor suitable for, SC
software; they argue that scientific developers naturally use an agile
philosophy~\cite{CarverEtAl2007, EasterbrookAndJohns2009,
  Segal2005} or an amethododical~\cite{Kelly2013}, or a knowledge acquisition
driven~\cite{Kelly2015}, process.  The arguments are that scientists do not
view rigid, process-heavy approaches, favourably~\cite{CarverEtAl2007} and that
in SC requirements are impossible to determine up-front~\cite{CarverEtAl2007,
  SegalAndMorris2008}.  Rather than abandon the benefits of a rational document
driven process, these arguments can be addressed through the development of
appropriate tools that support change and allow the scientist to focus on their
science.

% Scientific computing (SC) was the first application of computers. It is still
% used today for a wide variety of tasks: constructing mathematical models,
% performing quantitative analyses, creating simulations, solving scientific
% problems, etc. SC software has been developed for increasingly safety and
% security critical systems (nuclear reactor simulation, satellite
% guidance%D MORE EXAMPLES)
% ) as well as predictive
% systems.
% It has applications including (but not limited to) predicting weather patterns
% and natural disasters, and simulating economic fluctuations. As such, it is an
% incredibly important part of an increasing number of industries
% today.

% In the medical, nuclear power, aerospace, automotive, and manufacturing fields
% there are many safety critical systems in play. With each system, there is the
% possibility of a catastrophic failure endangering lives.  It is incredibly
% important then to have some means of certifying and assuring the quality of each
% software system. As Smith et al.~\cite{SmithAndKoothoor2016} stated
% ``Certification of Scientific Computing (SC) software is official recognition by
% an authority or regulatory body that the software is fit for its intended use.''
% These regulatory bodies determine certain certification standards that must be
% met for a system to become recognized as certified. One example of a
% certification standard is the Canadian Standards Association (CSA) requirements
% for quality assurance of scientific software for nuclear power plants.

% The main goal of software certification is to ``... systematically determine,
% based on the principles of science, engineering, and measurement theory, whether
% a software product satisfies accepted, well-defined and measurable criteria''
% \cite{HatcliffEtAl2009}. As such, certification would not only involve analyzing the code
% systematically and rigorously, but also analyzing the
% documentation. Essentially, this means the software must be both valid and
% verifiable, reliable, usable, maintainable, reusable, understandable, and
% reproducible. %D Verification involves ensuring that the software is ``solving
%               % the equations right'', whereas validation requires ensuring
%               % that
%               % the software is ``solving the right
%               % equations''\cite{Roache1998}.

% Developing certifiable software can end up being a much more involved process
% than developing uncertified software: it takes more money, time, and effort on
% the part of developers to produce. These increased costs lead to reluctance from
% practitioners to develop certifiable software \cite{Roache1998}. However, in
% our 
% opinion, cost is not the only contributing factor for the developers. As it
% stands in the field, scientists seem to prefer a more agile development process
% \cite{Segal2008}. However, this is not necessarily the best process as typically
% this would lead to problems maintaining the
% documentation, meaning that not all aspects of the software would be traceable
% through the design process and making certification more difficult (if not
% impossible).

% Given proper methods and tools, scientists can follow a structured approach
% (while capitalizing on frequent feedback and course correction) to meet
% documentation requirements and improve their overall productivity.  This is
% where our work comes in: 
Our goal is to eat our cake and have it too. We want to improve the qualities
(verifiability, reliability, usability etc.) of SC software and at the same time
improve, or at least not diminish, performance. Moreover, we want to improve
developer productivity to save time and money on SC software development,
certification and re-certification. To accomplish this we need to remove (by
hand generated) duplication between software artifacts for SC software
\cite{WilsonEtAl2013} and provide complete traceability between all software
artifacts. Practical accomplishment of these objectives means we need to provide
facilities for automatic software artifact generation. We aim to accomplish this
by having one ``source'' that captures all of the relevant knowledge for a given
SC problem. From this source we can generate all required documents and views.
That is, we aim to provide methods, tools and techniques to support a literate
process for developing scientific software that generalizes the idea behind
Knuth's~\cite{Knuth1984} literate programming.

%D May need to cut the roadmap for space.
In the following sections we  
%Section~\ref{sec:background} will give a more in-depth 
focus on SC software quality and literate
programming.
%Section~\ref{sec:lss} 
Then we introduce our framework, \fullname{} (\lss{}), 
%D Sous-Chef de Logiciel, Sous-Chef de l'Informatique?
%S While I like the creativity, I prefer LSS to SCS.  Let's see what Jacques thinks.
for improving SC software development using a structured approach.  We show
a short example of the framework and discuss its advantages. 
%Section~\ref{sec:todo} will discuss
We then discuss how we want the framework to evolve. % and what future work we
%intend to do. %Finally, Section~\ref{sec:conclusion}  
The last section provides concluding remarks.

\section{Background} \label{sec:background}

% There have been many challenges towards assuring the quality of software
% throughout the history of scientific computing. Many attempts have been made
% (some successful, others not) at overcoming these challenges and improving
% software quality. 
In this section we discuss challenges for developing SC software and we
introduce the ideas behind our approach.

\subsection{Challenges for Scientific Computing Software
  Quality} \label{ssec:challenges}

% SC software has certain characteristics which create challenges for its
% development. We will not discuss them all in depth, however, those of interest
% to us are the technique selection, input output (or understandability), and
% modification (or maintainability) challenges as described by Yu \cite{Yu2011}.

The \textit{technique selection challenge}~\cite{Yu2011} arises in SC because
the best numerical approach to solve a given problem is not known a priori.
Experimentation is inevitably necessary to determine the
appropriate order of interpolation, the degree of implicitness, etc. For a
framework for developing SC software to be successful, it should support
a separation of concerns between the physical model and the numerical algorithm.
Moreover, the framework should provide facilities for parameterizing the
expected algorithmic variabilities for easy experimentation with different
options.
%   is a challenge that
% comes up often when dealing with continuous mathematical equations. Since these
% cannot be solved directly (as computers are discrete), some technique must be
% chosen which can produce a solution which is ``good enough'' for the
% user. Typically this choice is left to domain experts as each technique will
% (not) satisfy certain non-functional
% requirements. %D NEEDS TO BE BROUGHT UP SOMEWHERE

The \textit{understandability} challenge~\cite{Yu2011} effects the source
code and the executable application. In an effort to make scientific
libraries and software as widely applicable as possible, most packages provide a
generic interface with a large number of options. The number of options
overwhelms users and causes programmers to not reuse libraries, since they do
not believe the interface needs to be as complicated as it
appears~\cite{Dubois2002}. To improve understandability, an ideal framework will
generate applications and libraries that are only as complicated as they need to
be for the job at hand.

% when impacts the usability of SC software
% and typically comes up where considerable amounts of input data are used to
% produce large volumes of output. The main issue in this case is the complicated
% nature of the input data and the output, leading developers to recreate
% (slightly modified) existing library routines to deal with the nature of the
% input and output.

The \textit{maintainability challenge}~\cite{Yu2011} comes up as requirements
change.  The high frequency of change for SC software especially causes problems
for certification. If the expense and time required for re-certification is on
the same order of magnitude as the original certification, changes will not be
made. To be effective in this environment, a framework needs to provide
traceability, so the consequences of change can be evaluated.
%  Changing requirements poses a
% problem for scientists: commercial software can be difficult/expensive to modify
% and noncommercial programs are not often flexible enough to change.

\subsection{Literate Programming} \label{ssec:literate}

Literate programming (LP) is a programming methodology introduced by Knuth
\cite{Knuth1984}. The main idea is to write programs in a way that explains (to
humans) what we want the computer to do, as opposed to simply giving the
computer instructions. % There is a focus on ordering the material to promote
% better human understanding.

In a literate program, the documentation and code are together in one source.
While developing a literate program, the algorithms used are broken down into
small, understandable parts (known as ``sections'' \cite{Knuth1984} or
``chunks'' \cite{JohnsonAndJohnson1997}) which are explained, documented, and
implemented in an order which promotes understanding. To get working source
code, the \textit{tangle} process is run, which extracts the code from the
literate document and reorders it into an appropriate structure for the computer
to understand. Similarly, the \textit{weave} process is run to extract and
typeset the documentation.
% By adhering to the LP methodology, a literate program ends up with documentation
% that is expertly written and of publishable quality. The code also ends up being
% well documented and high quality. 
There are several examples of SC programs being written in LP style, such as
VNODE-LP \cite{Nedialkov2006} and ``Physically Based Rendering: From Theory to
Implementation'' \cite{PharrAndHumphreys2004} (a literate program which is also
a textbook).

\section{Introducing \lss} \label{sec:lss} % (NEED A BETTER NAME)

Our framework, \lss, is being developed with two goals: complete traceability
throughout the development process and reduced knowledge duplication. Both of
these goals can be accomplished by generalizing a literate approach.

\subsection{The current state of \lss} \label{ssec:example}

\begin{figure}
	\includegraphics[width=0.45\textwidth]{h_c.pdf}
	\caption{SRS data definition of $h_c$}
	\label{fig:h_c}
\end{figure}	

\lss{} has been developed to this point using a practical, example-driven
approach. The first example used in guiding the development of \lss{} involves
the simplified Software Requirements Specification (SRS) for a fuel pin in a
nuclear reactor (See \cite{SmithAndKoothoor2016} for more details). For brevity
we look specifically at the term $h_c$ (defined in Figure~\ref{fig:h_c}).

At the time of this writing, we are able to generate the \verb|.tex| file for
much of the SRS for the fuel pin as well as the source code (in C) for the
required calculations.

\subsection{How it works} \label{sssec:ex_how}

The current framework is composed of several components including
\textit{chunks}, \textit{recipes}, and a \textit{generator}. The generator
produces views of the source material, where these views are the software
artifacts we require. Recipes are descriptions of these views and chunks contain
the source material.

% \begin{figure}
% \large{
% \Tree[.\fbox{Chunk(\textit{name})}
% 		[.\fbox{Concept(\textit{description})}
% 			[.\fbox{Quantity(\textit{symbol})} ]
% 			[.\fbox{Unit(\textit{unit})} ]
% 		]
% 	]
% }
% \caption{The chunk hierarchy design}
% \label{fig:chunks}
% \end{figure}

\begin{figure}
\begin{center}
{
 \includegraphics[width=0.35\textwidth]{ChunkHierarchy.pdf}
}
\end{center}
\caption{The chunk hierarchy design}
\label{fig:chunks}
\end{figure}

Each chunk needs to represent some concept, quantity, etc. Our current design
introduces a chunk hierarchy (as seen in Figure~\ref{fig:chunks}), where the
most basic chunk has only one field: a \textit{name}. A ``Concept'' adds a
\textit{description}, and so on.

Each chunk is built from one or more of the existing chunks. This can be seen in
the \verb|h_c| implementation below. Thus, all of the information related to any
one concept in a software project will be stored in a single chunk.

Recipes are specified using a micro- and macro-layout language embedded in
Haskell. Each describes how the generated artifacts should appear: The
micro-language handles the small-scale layout details (subscript and
superscripts, concatenation of symbols, etc.), whereas the macro-language
handles the larger layout details (sections, tables, etc.).

The source for our example has been broken down into the recipe, common
knowledge, and specific knowledge. A portion of the recipe can be seen in
Figure~\ref{fig:recipe}.

In this example, the fundamental SI units are common knowledge. Each is
contained within its own chunk in the SI unit library. As
Figure~\ref{fig:know_common} shows, each chunk has a name, description, and
symbolic representation.

\begin{figure}
\begin{lstlisting}[language=Haskell, frame=single, showstringspaces=false, basicstyle=\tiny]
srsBody = Document ((S "SRS for ") :+: 
    (N $ h_g ^. symbol) :+: 
    (S " and ") :+: (N $ h_c ^. symbol)) 
    (S "Spencer Smith") [s1,s2]

s1 = Section (S "Table of Units") 
    [s1_intro, s1_table]

s1_table = Table [S "Symbol", S "Description"] $ mkTable
    [(\x -> Sy (x ^. unit)),
     (\x -> S (x ^. descr))
    ] si_units

s1_intro = Paragraph (S "Throughout this ...
\end{lstlisting}
\caption{A portion of the SRS recipe}
\label{fig:recipe}
\end{figure}

\begin{figure}
\begin{lstlisting}[language=Haskell, frame=single, showstringspaces=false, basicstyle=\tiny]
metre, kilogram, second, kelvin, mole :: FundUnit
metre    = fund "Metre"    "length (metre)"       "m"
kilogram = fund "Kilogram" "mass (kilogram)"      "kg"
second   = fund "Second"   "time (second)"        "s"
kelvin   = fund "Kelvin"   "temperature (kelvin)" "K"
\end{lstlisting}
%mole     = fund "Mole"     "amount of substance (mole)"   "mol"
%ampere   = fund "Ampere"   "electric current (ampere)"    "A"
%candela  = fund "Candela"  "luminous intensity (candela)" "cd"
\caption{Segment of the SI unit library}
\label{fig:know_common}
\end{figure}

\begin{figure}
\begin{lstlisting}[language=Haskell, frame=single, showstringspaces=false, basicstyle=\small]
h_c_eq :: Expr
h_c_eq = ((Int 2):*(C k_c):*(C h_b)) :/ 
    ((Int 2):*(C k_c) :+ ((C tau_c):*(C h_b)))

h_c :: EqChunk
h_c = EC (UC (VC "h_c" 
    "convective heat transfer coefficient 
        between clad and coolant"
    (sub h c) ) heat_transfer) h_c_eq
\end{lstlisting}
\caption{The $h_c$ chunk}
\label{fig:know_specific}
\end{figure}

The $h_c$ chunk (Figure~\ref{fig:know_specific}) is a piece of specific
knowledge. It contains the name, description, symbol, units, and equation
(written in an internal expression language) for $h_c$.

The internal expression language used to implement the equation for $h_c$
%is implement the right word?
allows for the straighforward generation of source code. We utilize methods
similar to those found in \cite{Carette???, Szymczak???}
%self-source or is that tacky?
wherein the expression language is converted to an abstract representation of
the code and then passed to a pretty-printer to create the final source.

We currently generate C code, however it would be possible to generate any
language provided we have an appropriate representation for that language.
\subsection{Advantages} \label{ssec:advantages}

We can already see the beginning of some advantages over traditional SC
development. How \lss{} addresses the specific challenges of
Section~\ref{ssec:challenges} and will be explored below.

\subsubsection{Software Certification} \label{sssec:adv_cert}

High-quality documentation is required to certify software, but its creation
should not impede a scientists' work.  Major problems in creating SC software
stem from the maintainability and technique selection challenges
(Section~\ref{ssec:challenges}). As requirements and numerical algorithmic
decisions change, documentation and code must be updated.  This creates issues
with traceability and maintainability.

Depending on the regulatory body and the certification standards, many
types of documents may be required, such as requirements specification,
verification plans, design specification and code. \lss{} aims to generate these
documents alongside the code, while accounting for any changes. As the changes
will affect chunks and those chunks will be used to generate the documentation
and code, there is a guarantee that changes will propagate throughout all of the
artifacts.

% The following are examples of software artifacts that we wish to generate:
% \begin{enumerate}
% % \item Problem Statement
% %  A description of the problem to be solved, essentially
% %  summarizing the purpose of the software. There should be no mention of how the
% %  problem is to be solved.
% % \item Development Plan
% %  An overview of the development process and
% %  infrastructure. This should cover the documents to be created, which templates
% %  (if any) will be followed, internal processes to be employed, coding
% %  standards, and other details relevant to how the software will be developed.
% \item Software Requirements Specification (SRS)
% %  The desired functions and qualities of the
% %  software should be documented herein. The requirements should cover
% %  functionality, goals, context, design constraints, expected performance, and
% %  any other quality attributes of the software.
% \item Verification and Validation (V\&V) plan
% %  Documents how the documentation
% %  artifacts will be shown to be internally correct (verification) as well as how
% %  to ensure that the right problem has been solved (validation).
% \item Design Specification
% %  How will the requirements be realized? Document the
% %  software architecture and a detailed design of the modules/interfaces to be
% %  used. Can be thought of as a programmer's manual for maintenance.
% \item Code
% %  Implementation of the design specification.
% \item V\&V Report 
% %  Summarize the steps taken for
% %  performing verification and validation (this should mirror the V \& V plan
% %  from above) and include any testing results.
% % \item User Manual
% %  Full instructions on how the software is to be used,
% %  beginning with installation instructions and proceeding through detailed
% %  examples of the program's use.
% \end{enumerate}

% The artifacts listed above share non-trivial amounts of information, which is
% where traceability comes into play. To make any changes to the documentation,
% there must be some way to determine how a change will affect \textbf{all} of the
% artifacts. This is especially important for (re-) certification, as the
% documents and source must remain consistent.

In the context of re-certification, if a piece of software were developed using
\lss{} and some changes needed to be made, updating the artifacts and submitting
them for re-certification would be straightforward. All documentation would be
generated from the (newly modified) chunks, according to their existing recipes.
Or in the case of new information being added, a new chunk would be created and
the recipe slightly modified. During the implementation of our example, we have
already seen how \lss{} allows us to make changes at trivial cost.

On another note, if a document standard were to be changed during the
development cycle, it would not necessitate re-writing the entire document. All
of the information in the chunks would remain intact, only the recipe would need
to be changed to accommodate the new view. Thus, capturing all of the knowledge
that a program is based on and improving reproducibility.

\subsubsection{Knowledge Capture} \label{sssec:adv_knowledge}

In SC software there are many commonly shared theorems and formulae across
different applications. For example consider the general form of the
conservation of thermal energy equation. This equation is widely used in a
variety of thermal analysis applications, with each solving a different problem,
or modeling a different system.

Our approach aims to build libraries of chunks that can be reused anywhere. Each
library should contain common chunks relevant to a specific application domain
(ex. thermal analysis) and each project should aim to reuse as much as possible
during development.

From our example, a common source of reused knowledge is the Syst\'{e}me
International (SI) units (Figure~\ref{fig:know_common}). They are used in
applications throughout all of SC, so why should they be redefined for each
project? Once the knowledge has been captured, they can simply be reused
wherever necessary. With \lss{} they can be reused across projects with minimal
effort, allowing developers and scientists to spend their valuable time on more
important things.

%D This next section really feels like it's making a promise without really
%  explaining how/why it'll work.
\subsubsection{"Everything should be made as simple as possible, but not
  simpler."  (Einstein quote)} \label{sssec:adv_simple}

Currently there exist many powerful, general commercial programs for solving
problems using the finite element method. However, they are not often used to
develop new ``widgets'' because of the understandability challenge.
%cost and complexity of doing so~\cite{TODO}. %D Where is this from?
Engineers often have to resort to building and testing prototypes, instead of
performing simulations, due to a lack of tools that can assist with their exact
set of problems. %D Citation needed?

Our approach will change that by ideally making prototyping trivial through
generating source code suited to the needs of the engineers. Changes in
specifications could be seen in the code in (essentially) real-time at trivial
cost. For example, if an engineer were designing parts for strength, they could
have a general stress analysis program. This program could be 3D or specialized
for plane stress/strain, depending on which assumption would be most appropriate
at the time. The program could even be customized to the parameterized shape of
the part the engineer is interested in. The new program could only expose the
degrees of freedom necessary for the engineer to change (ex. material properties
or specific dimensions of the part), making the simulation process simpler and
safer.

\lss{} tackles this understandability challenge (Section~\ref{ssec:challenges})
by allowing developers to build components which will provide exactly what is
needed, no more and no less.

\subsubsection{Verification} \label{sssec:adv_verify}

When it comes to verification, requirements documents typically include
so-called ``sanity'' checks that can be reused throughout subsequent phases of
development. For instance, a requirement could assume conservation of mass or
constrain lengths to be always positive. The former would be used to test the
output and the latter to guard against invalid inputs.

With \lss, these sanity checks can be ensured by the knowledge capture
mechanism. Each chunk can maintain its own sanity checks and incorporate them
into the final system to ensure all inputs and outputs (including
intermediaries) are valid.

Also, with \lss{}, complete traceability is achievable through the use of
chunks, provided the requisite knowledge can be appropriately encapsulated.

Finally, any mistakes that occur in the generated software artifacts will occur
\textbf{everywhere}. Errors propagate through artifacts, and the artifacts will
always be in sync with each other (and the source). As a consequence, errors
will be much easier to find and only need to be fixed in one place.

\section{Future work} \label{sec:todo}

Currently the framework is still very small, producing only one document type
(the SRS) and only one type of code (C code for calculations). We plan to expand
\lss{} in several ways including, but not limited to:

\begin{enumerate}
\item Generate more artifact types. %(i.e. have more default recipes).
\item Generate different document views. %(ex. SRS with equation derivations).
\item Include more types of information in chunks (see Table\ref{tab:pcm}).
\item Auto-generate test cases using constraints and typical values. The
  constraints should determine error cases, and typical value ranges give
  warnings.
\item Continue to expand the tool by implementing larger example systems.
\end{enumerate}

\begin{table} \label{tab:pcm}
\centering
\caption{Future knowledge to capture}
\begin{tabular}{|c|c|r|c|} \hline
\textbf{Var} & \textbf{Constraints} & \textbf{Typical Value} & \textbf{Uncertainty}\\ \hline
$L$ & $L > 0$ & 1.5 m & 10\% \\ \hline
$D$ & $D > 0$ & 0.412 m & 10\% \\ \hline
$V_P$ & $V_P > 0$ (*)	& 0.05 m$^3$	& 10\% \\ \hline
$A_P$ & $A_P > 0$ (*)	& 1.2 m$^2$	& 10\% \\ \hline
$\rho_P$ & $\rho_P > 0$	& 1007 kg/m$^3$	& 10\% \\
\hline\end{tabular}
\end{table}

For the auto-generation of test cases, physical constraints will be seen as hard
limits on values (ex. length must always be positive and a negative value would
throw an error). Typical values, on the other hand, are ``reasonable'' values
(ex. the length of a beam should be on the order of several metres, but
theoretically it could be kilometres, thus the code will raise a warning instead
of an error).

    
\section{Concluding Remarks} \label{sec:conclusion}

The current standard of using agile approaches to SC software development leave
many things to be desired. Documents tend to fall out of sync with the source
with each iteration, and the amount of hand-duplicated information leads to
errors affecting the quality of the software.

We have begun the creation of a framework to help ensure complete traceability
between software artifacts in the development process, while attempting to
inconvenience the developers as little as possible. Using our framework will
hopefully lead to higher quality software at very little cost.


\bibliographystyle{abbrv}
\bibliography{sigproc}  
\end{document}
