\documentclass[sigconf, authorversion, nonacm]{acmart}

\settopmatter{printacmref=false}

% \makeatletter
% \def\@copyrightspace{\relax}
% \makeatother

% \setcopyright{none} 

\begin{document}

\title{Long-Term Productivity Based on Science, not Preference}

\author{Spencer Smith and Jacques Carette}
\orcid{0000-0002-0760-0987}
\affiliation{
  \department{Computing and Software, McMaster University, Canada}
  %\streetaddress{1280 Main Street West}
  %\institution{McMaster University}
  %\city{Hamilton}
  %\state{Ontario}
  %\postcode{L8S 4L8}
  %\country{Canada}
  }
\email{smiths@mcmaster.ca and carette@mcmaster.ca}

% \author{Jacques Carette}
% \orcid{0000-0001-8993-9804}
% \affiliation{
%   \department{Computing and Software}
%   %\streetaddress{1280 Main Street West}
%   \institution{McMaster University}
%   %\city{Hamilton}
%   %\state{Ontario}
%   %\postcode{L8S 4L8}
%   \country{Canada}}
% \email{carette@mcmaster.ca}

\maketitle

Our goal is to identify inhibitors and catalysts for productive long-term
scientific software development.  The inhibitors and catalysts could take the
form of processes, tools, techniques, and software artifacts (such as user
manuals, unit tests, design documents and code). The effort (time) invested in
catalysts will pay off in the long-term, while inhibitors will take up
resources, but instead of helping may actually lower product quality.

If developers are surveyed on inhibitors and catalysts, their answers will be as
varied as the education and experiential backgrounds of the respondents. Their
responses will be well-meaning, but they will undoubtedly come with problems and
biases.  For instance, developers may be guilty of the \emph{sunk cost fallacy},
promoting a technology that they have invested considerable hours in learning,
even if the current costs outweigh the benefits. As another example, developers
may recommend against proper requirements, but this lack of support doesn't
imply requirements are an inhibitor, only that current practice doesn't promote
this kind of documentation~\cite{HeatonAndCarver2015}. Another perceived
inhibitor is time spent in meetings. For instance, departmental retreats can be
unpopular because of a lack of short-term benefits, but relationship building
and strategic decision making may provide significant future rewards. The
difficult trick is to know which meetings are useful, and which are not.  As
these examples illustrate, we need to take developer and personal preference out
of the development process and instead pick the artifacts/processes/tools that
have a long-term impact.  We need to use a scientific approach based on
unambiguous definitions, empirical evidence, hypothesis testing and rigorous
processes.

\section{Building Blocks}

A scientific approach requires a solid foundation.  The building blocks for
scientific discourse is an unambiguous language for communicating concepts,
formulating hypotheses, planning data collection, and analyzing models and
theories.  To start with, we need to classify the software under discussion.
Likely dimensions for classification include the following: general purpose
scientific tools versus special purpose physical models, scientific domain, open
source versus commercial software, project maturity, project size, and level of
safety criticality.

Another important language component is defining what we hope to achieve in
terms of scientific software quality. Qualities that need to be unambiguously
defined include reliability, sustainability, reproducibility and productivity.
Software engineers have frequently attempted to define quality since the
1970s~\cite{McCallEtAl1977}, but the definitions aren't usually specific to
scientific software (as shown by the confusion between precision and accuracy is
the ISO/IEC definitions~\cite{ISO9126}). Moreover, the definitions often focus
on measurability, where the first priority should be conceptual clarity,
analogous to the unmeasurable, but conceptually clear, definition of (forward)
error, which requires knowing the, usually unknown, true answer.

For each relevant quality we recommend collecting as many distinct instances of
the definition as possible.  Once all the definitions are collected, they can be
assessed against the following criteria (based on~\citet{IEEE1998}):
completeness, consistency, modifiability, traceability, unambiguity and
abstractness. The understanding gained from this systematic survey and analysis
can then be used to propose a new set of quality definitions that allow for
reasoning about quality.

\section{Productivity}

- an example and an important quality. - Focus on definition of productivity,
using our long-term productivity for long-term impact
paper~\cite{SmithAndCarette2020arXiv}. 

As a starting point, here is our conceptual formula for productivity:

$$ I = \int_{0}^{T} H(t)\ dt $$
$$ O = \int_{0}^{T} \sum_{c \in C} S_c(t) K_c(t)\ dt $$
$$P = O / I$$ 

\noindent where $P$ is productivity, $I$ is the inputs, $O$ is the outputs, $0$
is the time the project started, $T$ is the time \emph{in the future} where we
want to take stock, $H$ is the total number of hours available by all personnel,
$C$ represents different classes of users (external as well as internal), $S$ is
satisfaction and $K$ is \emph{practical knowledge}.  Thus productivity is
measured in ``satisfying reusable knowledge per hour.''

\section{Measuring}

Interpretation of equation from previous section.

Measuring the impact of interventions on productivity (somewhat related to SOP
work and Olu’s work, but not something we’ve explored too much to date)

return on investment = (net program benefits / program costs) x 100

We adapt the standard definition of productivity [5], where inputs are labour,
but adjust the outputs to be knowledge and user satisfaction, where user
satisfaction acts as a proxy for effective quality. This explicit emphasis on
all knowledge produced, rather than just the operationalizable knowledge (aka
code) implies that human-reusable knowledge, i.e. documentation, is crucial.
This is why the long-lived context is important.

\section{State of the Practice}

Understanding the state of the practice, including finding a methodology
to assess the state of practice (SOP work) 

Understanding the gap between what is recommended and what is practiced (Olu’s
work) 

Research questions.

\section{Artifacts}

- potential artifacts: such as requirements, specifications, user manual, unit
tests, system tests, usability tests, build scripts, APIs, READMEs, license
documents, process documents, and code

- measuring the value of all of the artifacts - what do they contribute? (Output)
Determining what documentation should be produced based on what is necessary for
an assurance case argument for reliability (my work on assurance cases) 

\section{Production Methods}

- how do you produce the valued artifacts productively - how minimize cost in
hours (Input) Removing the drudgery of document generation and maintenance via
generation (Drasil) - de-syncronized artifacts - how measure that on knowledge -
keep knowledge and artifacts in-sync.
- de-sync as an inhibitor - maybe generative methods can solve
- sprinkle in idea

\section{Concluding Remarks}

- ROI

%% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{References}

\end{document}